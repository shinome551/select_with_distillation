# select_with_distillation

## 目的
全学習データを用いて学習したモデル（教師モデル）の出力をもとに、データの選択を行います。

データの選択は、そのサブセットを学習するモデル（生徒モデル）の精度が出来るだけ教師モデルに近づくように行います。

精度を維持しながらどれだけデータセットを削減できるか、というのが目標です。

## 手順
基本的には、教師モデルが出力する各データの交差エントロピー損失を指標とし、その降順に任意の数のデータを取り出すことで学習データのサブセットを構築します。

select.pyではそのシンプルな実装です。しかし、この場合はサブセットのサイズが小さい（10000）場合ときに精度がガクッと落ちます。いわゆる過学習と呼ばれるものが発生していると考えられます。

そこで、select_with_distillation.pyでは知識の蒸留（knowledge distillation）[1]を用いて正則化を行っています。教師モデルは既に存在しますから、使えるものは使いましょう。

蒸留を合わせて使うことで、サブセットのサイズが小さい場合でも精度の劣化は抑えられます。ついでに教師モデルの精度を超える場合があることもわかります。この現象は蒸留ではよく見られるものですが、不思議ですね。

## 実行結果
データセットはCIFAR10を用いています。また、蒸留において各モデルの出力を調節する温度Tを変えて実行しました。

・T=1の場合
| sample num    | Random Acc@1 | Select Acc@1 |
|---------------|:------------:|:------------:|
| 50000         |    91.7      |              |
| 40000         |    90.2      |    90.8      |
| 30000         |    88.8      |    89.9      |
| 20000         |    86.7      |    87.7      |
| 10000         |    82.1      |    79.0      |

・T=20の場合
| sample num    | Random Acc@1 | Select Acc@1 |
|---------------|--------------|--------------|
| 50000         |    92.3      |              |
| 40000         |    91.8      |    92.0      |
| 30000         |    91.3      |    91.6      |
| 20000         |    90.5      |    90.9      |
| 10000         |    87.4      |    86.8      |

## モデル
教師モデルにはGitHub上で公開されている学習済みのResnet20を使います([Link](https://github.com/chenyaofo/CIFAR-pretrained-models))。

また、生徒モデルにも同じモデルを使用しています。

後日自作のものに置き換えるかもしれません。

```
$ git clone https://github.com/chenyaofo/CIFAR-pretrained-models
```

## TODO
arg parseでパラメータをいじれるようにする

## Reference
1. Hinton, Geoffrey, Oriol Vinyals, and Jeff Dean. "Distilling the knowledge in a neural network." arXiv preprint arXiv:1503.02531 (2015).

[1]: https://arxiv.org/abs/1503.02531
